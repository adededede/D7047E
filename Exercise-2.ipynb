{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Practical\n",
    "In this task you are going to observe the effect of transfer learning on a (once) very popular architecture.\n",
    ">**NOTE:** Before getting started make sure you have the required packages installed along with a suitable IDE you want to work on. Also if your computer doesnâ€™t supports these installations, you can also work on Google Colab. Here Google provides computational capacity (to some extent) for running deep learning codes. These are same as Jupyter Notebooks.\n",
    "### 1.1 Task\n",
    "#### 1.1.1 Transfer Learning from ImageNet\n",
    "- Download and prepare CIFAR-10 dataset (it is already available in the above mentioned libraries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Langage\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Langage\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Langage\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#loading the Cifar-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "#preparing the data\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "#reshaping\n",
    "x_train= (x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3))/ 255.0\n",
    "x_test = (x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3))/ 255.0\n",
    "\n",
    "#converting label to one-hot encoding\n",
    "y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
    "y_test = tf.one_hot(y_test.astype(np.int32), depth=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use AlexNet as the model (Pytorch AlexNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "#AlexNet model definition\n",
    "alexnet = Sequential()\n",
    "alexnet.add(Conv2D(filters=64, kernel_size=(11,11), strides=(4,4), padding='same', activation='relu', input_shape=(32,32,3)))\n",
    "alexnet.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "alexnet.add(Conv2D(filters=192, kernel_size=(5,5), strides=(1,1), padding='same', activation='relu'))\n",
    "alexnet.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "alexnet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "alexnet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "alexnet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "alexnet.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))\n",
    "alexnet.add(Flatten())\n",
    "alexnet.add(Dense(units=4096, activation='relu'))\n",
    "alexnet.add(Dropout(0.5))\n",
    "alexnet.add(Dense(units=4096, activation='relu'))\n",
    "alexnet.add(Dropout(0.5))\n",
    "#extra fully connected layer with 10 neurons\n",
    "alexnet.add(Dense(units=10, activation='softmax'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have to perform two separate experiments\n",
    "> * Train the model for CIFAR-10 data, Report the test test accuracy. (also referred as finetuning the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 319s 809ms/step - loss: 1.9815 - accuracy: 0.2102 - val_loss: 1.8200 - val_accuracy: 0.2931\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 308s 787ms/step - loss: 1.7247 - accuracy: 0.3338 - val_loss: 1.6286 - val_accuracy: 0.3745\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 328s 838ms/step - loss: 1.5796 - accuracy: 0.4104 - val_loss: 1.5235 - val_accuracy: 0.4339\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 336s 858ms/step - loss: 1.4734 - accuracy: 0.4600 - val_loss: 1.4433 - val_accuracy: 0.4731\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 357s 915ms/step - loss: 1.4074 - accuracy: 0.4885 - val_loss: 1.3961 - val_accuracy: 0.4931\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 347s 888ms/step - loss: 1.3393 - accuracy: 0.5186 - val_loss: 1.3251 - val_accuracy: 0.5276\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 316s 808ms/step - loss: 1.2902 - accuracy: 0.5393 - val_loss: 1.3113 - val_accuracy: 0.5319\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 349s 894ms/step - loss: 1.2534 - accuracy: 0.5546 - val_loss: 1.2954 - val_accuracy: 0.5423\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 332s 849ms/step - loss: 1.2122 - accuracy: 0.5698 - val_loss: 1.2608 - val_accuracy: 0.5560\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 325s 832ms/step - loss: 1.1690 - accuracy: 0.5919 - val_loss: 1.2755 - val_accuracy: 0.5528\n",
      "Test accuracy: 0.5527999997138977\n"
     ]
    }
   ],
   "source": [
    "#compiling the model\n",
    "alexnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#training the model\n",
    "historic = alexnet.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "#reporting the accuracy\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(historic.history['loss'], color='b', label=\"Training Loss\")\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "ax[1].plot(historic.history['accuracy'], color='b', label=\"Training Accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)\n",
    "test_loss, test_acc = alexnet.evaluate(x_test, y_test, verbose=0)\n",
    "plt.show()\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Use the pretrained weights of AlexNet, in other words use AlexNet as a pretrained network for image classification on CIFAR-10 data (also referred as Feature Extraction), Report the test test accuracy.    \n",
    "- In both the above cases remember to add an extra fully connected layer to the classifier with number of neurons = 10, because there are 10 classes in CIFAR-10 dataset. This layer will be trainable in both the cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "391/391 [==============================] - 39s 98ms/step - loss: 2.3016 - accuracy: 0.1109 - val_loss: 2.3007 - val_accuracy: 0.1292\n",
      "Epoch 2/30\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 2.2995 - accuracy: 0.1163 - val_loss: 2.2989 - val_accuracy: 0.1577\n",
      "Epoch 3/30\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 2.2975 - accuracy: 0.1329 - val_loss: 2.2972 - val_accuracy: 0.1703\n",
      "Epoch 4/30\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 2.2955 - accuracy: 0.1433 - val_loss: 2.2955 - val_accuracy: 0.1657\n",
      "Epoch 5/30\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 2.2936 - accuracy: 0.1551 - val_loss: 2.2939 - val_accuracy: 0.1667\n",
      "Epoch 6/30\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 2.2916 - accuracy: 0.1484 - val_loss: 2.2923 - val_accuracy: 0.1778\n",
      "Epoch 7/30\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.2897 - accuracy: 0.1663 - val_loss: 2.2908 - val_accuracy: 0.1342\n",
      "Epoch 8/30\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 2.2879 - accuracy: 0.1705 - val_loss: 2.2894 - val_accuracy: 0.1508\n",
      "Epoch 9/30\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 2.2861 - accuracy: 0.1681 - val_loss: 2.2880 - val_accuracy: 0.1608\n",
      "Epoch 10/30\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 2.2843 - accuracy: 0.1717 - val_loss: 2.2866 - val_accuracy: 0.1487\n",
      "Epoch 11/30\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 2.2826 - accuracy: 0.1772 - val_loss: 2.2853 - val_accuracy: 0.1329\n",
      "Epoch 12/30\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 2.2810 - accuracy: 0.1620 - val_loss: 2.2838 - val_accuracy: 0.1555\n",
      "Epoch 13/30\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 2.2794 - accuracy: 0.1802 - val_loss: 2.2827 - val_accuracy: 0.1561\n",
      "Epoch 14/30\n",
      "391/391 [==============================] - 42s 106ms/step - loss: 2.2777 - accuracy: 0.1814 - val_loss: 2.2815 - val_accuracy: 0.1361\n",
      "Epoch 15/30\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 2.2762 - accuracy: 0.1758 - val_loss: 2.2804 - val_accuracy: 0.1471\n",
      "Epoch 16/30\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 2.2747 - accuracy: 0.1770 - val_loss: 2.2792 - val_accuracy: 0.1376\n",
      "Epoch 17/30\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 2.2730 - accuracy: 0.1763 - val_loss: 2.2783 - val_accuracy: 0.1372\n",
      "Epoch 18/30\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 2.2715 - accuracy: 0.1815 - val_loss: 2.2773 - val_accuracy: 0.1484\n",
      "Epoch 19/30\n",
      "391/391 [==============================] - 35s 91ms/step - loss: 2.2702 - accuracy: 0.1756 - val_loss: 2.2764 - val_accuracy: 0.1459\n",
      "Epoch 20/30\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 2.2687 - accuracy: 0.1803 - val_loss: 2.2754 - val_accuracy: 0.1387\n",
      "Epoch 21/30\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 2.2673 - accuracy: 0.1869 - val_loss: 2.2743 - val_accuracy: 0.1432\n",
      "Epoch 22/30\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 2.2661 - accuracy: 0.1786 - val_loss: 2.2737 - val_accuracy: 0.1463\n",
      "Epoch 23/30\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 2.2646 - accuracy: 0.1855 - val_loss: 2.2726 - val_accuracy: 0.1475\n",
      "Epoch 24/30\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 2.2635 - accuracy: 0.1753 - val_loss: 2.2714 - val_accuracy: 0.1509\n",
      "Epoch 25/30\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 2.2620 - accuracy: 0.1850 - val_loss: 2.2709 - val_accuracy: 0.1454\n",
      "Epoch 26/30\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 2.2608 - accuracy: 0.1834 - val_loss: 2.2706 - val_accuracy: 0.1433\n",
      "Epoch 27/30\n",
      "391/391 [==============================] - 40s 101ms/step - loss: 2.2594 - accuracy: 0.1816 - val_loss: 2.2695 - val_accuracy: 0.1410\n",
      "Epoch 28/30\n",
      "391/391 [==============================] - 40s 101ms/step - loss: 2.2584 - accuracy: 0.1863 - val_loss: 2.2691 - val_accuracy: 0.1518\n",
      "Epoch 29/30\n",
      "391/391 [==============================] - 42s 106ms/step - loss: 2.2571 - accuracy: 0.1867 - val_loss: 2.2680 - val_accuracy: 0.1430\n",
      "Epoch 30/30\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 2.2559 - accuracy: 0.1910 - val_loss: 2.2676 - val_accuracy: 0.1468\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.1467999964952469\n"
     ]
    }
   ],
   "source": [
    "#useless right?\n",
    "#saving the weights \n",
    "alexnet.save_weights('alexnet_weights.h5')\n",
    "#loading the weights\n",
    "alexnet.load_weights('alexnet_weights.h5')\n",
    "#create a new model that consists of all layers of our alexnet\n",
    "feature_extractor = Sequential()\n",
    "for layer in alexnet.layers[:-1]:\n",
    "    feature_extractor.add(layer)\n",
    "\n",
    "#freezing the weights of all layers\n",
    "for layer in feature_extractor.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#adding a new fully connected layer on top of the feature extractor for classification\n",
    "feature_extractor.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "#compiling the model\n",
    "feature_extractor.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#training the new fully connected layer on top of the feature extractor\n",
    "#using the CIFAR-10 data\n",
    "history = feature_extractor.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_test, y_test))\n",
    "\n",
    "#evaluating the accuracy of the feature extractor on the test set\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training Accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)\n",
    "test_loss, test_acc = feature_extractor.evaluate(x_test, y_test, verbose=0)\n",
    "plt.show()\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain (briefly!) what is the difference between the two runs and why there is a difference in performance.\\\n",
    "**ANSWER:**\\\n",
    "In the first experiment we have initializes randomly the weights and then we have optimized them through the training with our CIFAR-10 dataset. Which made it really long to compute (55 minutes for 10 epochs).\\\n",
    "In the second experiment we used our pre-trained weights, since the weights have already been optimized it should be easier for our network to learn features, here we only retrain the last layer. Plus, this time it was way faster to compute (6 minutes for 10 epochs).\\\n",
    "Since we use the exact same datasets in both experiment the accuracy is getting worth, this could be due to overfitting. I tried incrising the epochs from  10 to 30 but it didn't change the loss that much, neither the accuracy.\n",
    "#### 1.1.2 Transfer Learning from MNIST\n",
    "- Prepare a CNN of your choice and train it on the MNIST data. Report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/937], Loss: 0.2906\n",
      "Epoch [1/10], Step [200/937], Loss: 0.1342\n",
      "Epoch [1/10], Step [300/937], Loss: 0.0544\n",
      "Epoch [1/10], Step [400/937], Loss: 0.0179\n",
      "Epoch [1/10], Step [500/937], Loss: 0.0587\n",
      "Epoch [1/10], Step [600/937], Loss: 0.1897\n",
      "Epoch [1/10], Step [700/937], Loss: 0.0410\n",
      "Epoch [1/10], Step [800/937], Loss: 0.2260\n",
      "Epoch [1/10], Step [900/937], Loss: 0.0621\n",
      "Epoch [2/10], Step [100/937], Loss: 0.0686\n",
      "Epoch [2/10], Step [200/937], Loss: 0.0358\n",
      "Epoch [2/10], Step [300/937], Loss: 0.0669\n",
      "Epoch [2/10], Step [400/937], Loss: 0.0107\n",
      "Epoch [2/10], Step [500/937], Loss: 0.0435\n",
      "Epoch [2/10], Step [600/937], Loss: 0.1064\n",
      "Epoch [2/10], Step [700/937], Loss: 0.0477\n",
      "Epoch [2/10], Step [800/937], Loss: 0.0151\n",
      "Epoch [2/10], Step [900/937], Loss: 0.0372\n",
      "Epoch [3/10], Step [100/937], Loss: 0.1281\n",
      "Epoch [3/10], Step [200/937], Loss: 0.0046\n",
      "Epoch [3/10], Step [300/937], Loss: 0.0094\n",
      "Epoch [3/10], Step [400/937], Loss: 0.1268\n",
      "Epoch [3/10], Step [500/937], Loss: 0.0090\n",
      "Epoch [3/10], Step [600/937], Loss: 0.0022\n",
      "Epoch [3/10], Step [700/937], Loss: 0.0193\n",
      "Epoch [3/10], Step [800/937], Loss: 0.0147\n",
      "Epoch [3/10], Step [900/937], Loss: 0.0148\n",
      "Epoch [4/10], Step [100/937], Loss: 0.0310\n",
      "Epoch [4/10], Step [200/937], Loss: 0.0045\n",
      "Epoch [4/10], Step [300/937], Loss: 0.0008\n",
      "Epoch [4/10], Step [400/937], Loss: 0.1605\n",
      "Epoch [4/10], Step [500/937], Loss: 0.0035\n",
      "Epoch [4/10], Step [600/937], Loss: 0.0763\n",
      "Epoch [4/10], Step [700/937], Loss: 0.0034\n",
      "Epoch [4/10], Step [800/937], Loss: 0.0874\n",
      "Epoch [4/10], Step [900/937], Loss: 0.0112\n",
      "Epoch [5/10], Step [100/937], Loss: 0.0464\n",
      "Epoch [5/10], Step [200/937], Loss: 0.0079\n",
      "Epoch [5/10], Step [300/937], Loss: 0.0072\n",
      "Epoch [5/10], Step [400/937], Loss: 0.0005\n",
      "Epoch [5/10], Step [500/937], Loss: 0.0032\n",
      "Epoch [5/10], Step [600/937], Loss: 0.0668\n",
      "Epoch [5/10], Step [700/937], Loss: 0.0252\n",
      "Epoch [5/10], Step [800/937], Loss: 0.0003\n",
      "Epoch [5/10], Step [900/937], Loss: 0.0657\n",
      "Epoch [6/10], Step [100/937], Loss: 0.0160\n",
      "Epoch [6/10], Step [200/937], Loss: 0.0003\n",
      "Epoch [6/10], Step [300/937], Loss: 0.0164\n",
      "Epoch [6/10], Step [400/937], Loss: 0.0032\n",
      "Epoch [6/10], Step [500/937], Loss: 0.0028\n",
      "Epoch [6/10], Step [600/937], Loss: 0.0016\n",
      "Epoch [6/10], Step [700/937], Loss: 0.0019\n",
      "Epoch [6/10], Step [800/937], Loss: 0.0005\n",
      "Epoch [6/10], Step [900/937], Loss: 0.0010\n",
      "Epoch [7/10], Step [100/937], Loss: 0.0003\n",
      "Epoch [7/10], Step [200/937], Loss: 0.0011\n",
      "Epoch [7/10], Step [300/937], Loss: 0.0080\n",
      "Epoch [7/10], Step [400/937], Loss: 0.0016\n",
      "Epoch [7/10], Step [500/937], Loss: 0.0052\n",
      "Epoch [7/10], Step [600/937], Loss: 0.0006\n",
      "Epoch [7/10], Step [700/937], Loss: 0.0132\n",
      "Epoch [7/10], Step [800/937], Loss: 0.0043\n",
      "Epoch [7/10], Step [900/937], Loss: 0.0049\n",
      "Epoch [8/10], Step [100/937], Loss: 0.0001\n",
      "Epoch [8/10], Step [200/937], Loss: 0.0021\n",
      "Epoch [8/10], Step [300/937], Loss: 0.0001\n",
      "Epoch [8/10], Step [400/937], Loss: 0.0001\n",
      "Epoch [8/10], Step [500/937], Loss: 0.0001\n",
      "Epoch [8/10], Step [600/937], Loss: 0.0006\n",
      "Epoch [8/10], Step [700/937], Loss: 0.0006\n",
      "Epoch [8/10], Step [800/937], Loss: 0.0003\n",
      "Epoch [8/10], Step [900/937], Loss: 0.0472\n",
      "Epoch [9/10], Step [100/937], Loss: 0.0012\n",
      "Epoch [9/10], Step [200/937], Loss: 0.0002\n",
      "Epoch [9/10], Step [300/937], Loss: 0.0004\n",
      "Epoch [9/10], Step [400/937], Loss: 0.0150\n",
      "Epoch [9/10], Step [500/937], Loss: 0.0005\n",
      "Epoch [9/10], Step [600/937], Loss: 0.0065\n",
      "Epoch [9/10], Step [700/937], Loss: 0.0000\n",
      "Epoch [9/10], Step [800/937], Loss: 0.0008\n",
      "Epoch [9/10], Step [900/937], Loss: 0.0005\n",
      "Epoch [10/10], Step [100/937], Loss: 0.0001\n",
      "Epoch [10/10], Step [200/937], Loss: 0.0004\n",
      "Epoch [10/10], Step [300/937], Loss: 0.0000\n",
      "Epoch [10/10], Step [400/937], Loss: 0.0016\n",
      "Epoch [10/10], Step [500/937], Loss: 0.0003\n",
      "Epoch [10/10], Step [600/937], Loss: 0.0005\n",
      "Epoch [10/10], Step [700/937], Loss: 0.0027\n",
      "Epoch [10/10], Step [800/937], Loss: 0.0010\n",
      "Epoch [10/10], Step [900/937], Loss: 0.0095\n",
      "Step [10/156], Test Accuracy: 98.91%\n",
      "Step [20/156], Test Accuracy: 98.67%\n",
      "Step [30/156], Test Accuracy: 98.65%\n",
      "Step [40/156], Test Accuracy: 98.63%\n",
      "Step [50/156], Test Accuracy: 98.56%\n",
      "Step [60/156], Test Accuracy: 98.46%\n",
      "Step [70/156], Test Accuracy: 98.59%\n",
      "Step [80/156], Test Accuracy: 98.61%\n",
      "Step [90/156], Test Accuracy: 98.72%\n",
      "Step [100/156], Test Accuracy: 98.78%\n",
      "Step [110/156], Test Accuracy: 98.81%\n",
      "Step [120/156], Test Accuracy: 98.88%\n",
      "Step [130/156], Test Accuracy: 98.93%\n",
      "Step [140/156], Test Accuracy: 98.96%\n",
      "Step [150/156], Test Accuracy: 99.02%\n",
      "Test Accuracy: 98.98%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(7*7*32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.pool(self.conv1(x)))\n",
    "        x = nn.functional.relu(self.pool(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "# Define the dataset and data loader\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=False)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transform, download=False)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, len(train_dataset)//train_loader.batch_size, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Step [{}/{}], Test Accuracy: {:.2f}%'.format(i+1, len(test_dataset)//test_loader.batch_size, 100 * correct / total))\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print('Test Accuracy: {:.2f}%'.format(test_accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the above model as a pretrained CNN for the SVHN dataset. Report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Langage\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Langage\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data\\train_32x32.mat\n",
      "Using downloaded and verified file: data\\test_32x32.mat\n",
      "Epoch [1/10], Step [100/1144], Loss: 2.0868\n",
      "Epoch [1/10], Step [200/1144], Loss: 2.2299\n",
      "Epoch [1/10], Step [300/1144], Loss: 2.0100\n",
      "Epoch [1/10], Step [400/1144], Loss: 2.1626\n",
      "Epoch [1/10], Step [500/1144], Loss: 2.0231\n",
      "Epoch [1/10], Step [600/1144], Loss: 1.9958\n",
      "Epoch [1/10], Step [700/1144], Loss: 2.0237\n",
      "Epoch [1/10], Step [800/1144], Loss: 1.8927\n",
      "Epoch [1/10], Step [900/1144], Loss: 1.9483\n",
      "Epoch [1/10], Step [1000/1144], Loss: 1.9941\n",
      "Epoch [1/10], Step [1100/1144], Loss: 2.0711\n",
      "Epoch [2/10], Step [100/1144], Loss: 2.0547\n",
      "Epoch [2/10], Step [200/1144], Loss: 1.7924\n",
      "Epoch [2/10], Step [300/1144], Loss: 1.8647\n",
      "Epoch [2/10], Step [400/1144], Loss: 2.0176\n",
      "Epoch [2/10], Step [500/1144], Loss: 2.0865\n",
      "Epoch [2/10], Step [600/1144], Loss: 2.0138\n",
      "Epoch [2/10], Step [700/1144], Loss: 1.9101\n",
      "Epoch [2/10], Step [800/1144], Loss: 2.2036\n",
      "Epoch [2/10], Step [900/1144], Loss: 1.8108\n",
      "Epoch [2/10], Step [1000/1144], Loss: 1.9838\n",
      "Epoch [2/10], Step [1100/1144], Loss: 1.7699\n",
      "Epoch [3/10], Step [100/1144], Loss: 1.8926\n",
      "Epoch [3/10], Step [200/1144], Loss: 1.9911\n",
      "Epoch [3/10], Step [300/1144], Loss: 1.8617\n",
      "Epoch [3/10], Step [400/1144], Loss: 1.9459\n",
      "Epoch [3/10], Step [500/1144], Loss: 1.9053\n",
      "Epoch [3/10], Step [600/1144], Loss: 1.9963\n",
      "Epoch [3/10], Step [700/1144], Loss: 2.0650\n",
      "Epoch [3/10], Step [800/1144], Loss: 2.1650\n",
      "Epoch [3/10], Step [900/1144], Loss: 1.8984\n",
      "Epoch [3/10], Step [1000/1144], Loss: 1.7450\n",
      "Epoch [3/10], Step [1100/1144], Loss: 1.8492\n",
      "Epoch [4/10], Step [100/1144], Loss: 1.9820\n",
      "Epoch [4/10], Step [200/1144], Loss: 2.0167\n",
      "Epoch [4/10], Step [300/1144], Loss: 1.9822\n",
      "Epoch [4/10], Step [400/1144], Loss: 1.8230\n",
      "Epoch [4/10], Step [500/1144], Loss: 1.8512\n",
      "Epoch [4/10], Step [600/1144], Loss: 1.8559\n",
      "Epoch [4/10], Step [700/1144], Loss: 2.0020\n",
      "Epoch [4/10], Step [800/1144], Loss: 1.7345\n",
      "Epoch [4/10], Step [900/1144], Loss: 1.8753\n",
      "Epoch [4/10], Step [1000/1144], Loss: 1.9187\n",
      "Epoch [4/10], Step [1100/1144], Loss: 2.0730\n",
      "Epoch [5/10], Step [100/1144], Loss: 1.8502\n",
      "Epoch [5/10], Step [200/1144], Loss: 1.7849\n",
      "Epoch [5/10], Step [300/1144], Loss: 1.6564\n",
      "Epoch [5/10], Step [400/1144], Loss: 1.9470\n",
      "Epoch [5/10], Step [500/1144], Loss: 1.9815\n",
      "Epoch [5/10], Step [600/1144], Loss: 2.0908\n",
      "Epoch [5/10], Step [700/1144], Loss: 1.7337\n",
      "Epoch [5/10], Step [800/1144], Loss: 1.8745\n",
      "Epoch [5/10], Step [900/1144], Loss: 2.0213\n",
      "Epoch [5/10], Step [1000/1144], Loss: 2.0175\n",
      "Epoch [5/10], Step [1100/1144], Loss: 1.9627\n",
      "Epoch [6/10], Step [100/1144], Loss: 1.8864\n",
      "Epoch [6/10], Step [200/1144], Loss: 1.8606\n",
      "Epoch [6/10], Step [300/1144], Loss: 2.1153\n",
      "Epoch [6/10], Step [400/1144], Loss: 1.9827\n",
      "Epoch [6/10], Step [500/1144], Loss: 1.9839\n",
      "Epoch [6/10], Step [600/1144], Loss: 1.7374\n",
      "Epoch [6/10], Step [700/1144], Loss: 1.7549\n",
      "Epoch [6/10], Step [800/1144], Loss: 2.1153\n",
      "Epoch [6/10], Step [900/1144], Loss: 2.0480\n",
      "Epoch [6/10], Step [1000/1144], Loss: 1.9071\n",
      "Epoch [6/10], Step [1100/1144], Loss: 1.9330\n",
      "Epoch [7/10], Step [100/1144], Loss: 1.6869\n",
      "Epoch [7/10], Step [200/1144], Loss: 2.0852\n",
      "Epoch [7/10], Step [300/1144], Loss: 1.9144\n",
      "Epoch [7/10], Step [400/1144], Loss: 1.8471\n",
      "Epoch [7/10], Step [500/1144], Loss: 1.8319\n",
      "Epoch [7/10], Step [600/1144], Loss: 2.1062\n",
      "Epoch [7/10], Step [700/1144], Loss: 1.9557\n",
      "Epoch [7/10], Step [800/1144], Loss: 2.0584\n",
      "Epoch [7/10], Step [900/1144], Loss: 1.9910\n",
      "Epoch [7/10], Step [1000/1144], Loss: 2.1423\n",
      "Epoch [7/10], Step [1100/1144], Loss: 1.9656\n",
      "Epoch [8/10], Step [100/1144], Loss: 1.9961\n",
      "Epoch [8/10], Step [200/1144], Loss: 1.9075\n",
      "Epoch [8/10], Step [300/1144], Loss: 2.0225\n",
      "Epoch [8/10], Step [400/1144], Loss: 2.0136\n",
      "Epoch [8/10], Step [500/1144], Loss: 2.0126\n",
      "Epoch [8/10], Step [600/1144], Loss: 2.1176\n",
      "Epoch [8/10], Step [700/1144], Loss: 1.8319\n",
      "Epoch [8/10], Step [800/1144], Loss: 2.0011\n",
      "Epoch [8/10], Step [900/1144], Loss: 2.0315\n",
      "Epoch [8/10], Step [1000/1144], Loss: 1.9364\n",
      "Epoch [8/10], Step [1100/1144], Loss: 2.0336\n",
      "Epoch [9/10], Step [100/1144], Loss: 2.0285\n",
      "Epoch [9/10], Step [200/1144], Loss: 1.9530\n",
      "Epoch [9/10], Step [300/1144], Loss: 2.1184\n",
      "Epoch [9/10], Step [400/1144], Loss: 1.9531\n",
      "Epoch [9/10], Step [500/1144], Loss: 2.2284\n",
      "Epoch [9/10], Step [600/1144], Loss: 1.9975\n",
      "Epoch [9/10], Step [700/1144], Loss: 1.8887\n",
      "Epoch [9/10], Step [800/1144], Loss: 1.8727\n",
      "Epoch [9/10], Step [900/1144], Loss: 1.9938\n",
      "Epoch [9/10], Step [1000/1144], Loss: 1.9450\n",
      "Epoch [9/10], Step [1100/1144], Loss: 1.9205\n",
      "Epoch [10/10], Step [100/1144], Loss: 1.8014\n",
      "Epoch [10/10], Step [200/1144], Loss: 1.9145\n",
      "Epoch [10/10], Step [300/1144], Loss: 2.0014\n",
      "Epoch [10/10], Step [400/1144], Loss: 2.0450\n",
      "Epoch [10/10], Step [500/1144], Loss: 2.1123\n",
      "Epoch [10/10], Step [600/1144], Loss: 2.1024\n",
      "Epoch [10/10], Step [700/1144], Loss: 1.8506\n",
      "Epoch [10/10], Step [800/1144], Loss: 1.8494\n",
      "Epoch [10/10], Step [900/1144], Loss: 1.8041\n",
      "Epoch [10/10], Step [1000/1144], Loss: 1.9014\n",
      "Epoch [10/10], Step [1100/1144], Loss: 2.1074\n",
      "Test Accuracy of the model on the 640 test images: 31.09375 %\n",
      "Test Accuracy of the model on the 1280 test images: 31.328125 %\n",
      "Test Accuracy of the model on the 1920 test images: 30.989583333333332 %\n",
      "Test Accuracy of the model on the 2560 test images: 32.1875 %\n",
      "Test Accuracy of the model on the 3200 test images: 31.46875 %\n",
      "Test Accuracy of the model on the 3840 test images: 31.40625 %\n",
      "Test Accuracy of the model on the 4480 test images: 31.696428571428573 %\n",
      "Test Accuracy of the model on the 5120 test images: 32.01171875 %\n",
      "Test Accuracy of the model on the 5760 test images: 32.135416666666664 %\n",
      "Test Accuracy of the model on the 6400 test images: 32.40625 %\n",
      "Test Accuracy of the model on the 7040 test images: 32.25852272727273 %\n",
      "Test Accuracy of the model on the 7680 test images: 32.122395833333336 %\n",
      "Test Accuracy of the model on the 8320 test images: 32.17548076923077 %\n",
      "Test Accuracy of the model on the 8960 test images: 32.098214285714285 %\n",
      "Test Accuracy of the model on the 9600 test images: 32.1875 %\n",
      "Test Accuracy of the model on the 10240 test images: 32.36328125 %\n",
      "Test Accuracy of the model on the 10880 test images: 32.45404411764706 %\n",
      "Test Accuracy of the model on the 11520 test images: 32.482638888888886 %\n",
      "Test Accuracy of the model on the 12160 test images: 32.4671052631579 %\n",
      "Test Accuracy of the model on the 12800 test images: 32.375 %\n",
      "Test Accuracy of the model on the 13440 test images: 32.410714285714285 %\n",
      "Test Accuracy of the model on the 14080 test images: 32.44318181818182 %\n",
      "Test Accuracy of the model on the 14720 test images: 32.37092391304348 %\n",
      "Test Accuracy of the model on the 15360 test images: 32.389322916666664 %\n",
      "Test Accuracy of the model on the 16000 test images: 32.44375 %\n",
      "Test Accuracy of the model on the 16640 test images: 32.56009615384615 %\n",
      "Test Accuracy of the model on the 17280 test images: 32.604166666666664 %\n",
      "Test Accuracy of the model on the 17920 test images: 32.505580357142854 %\n",
      "Test Accuracy of the model on the 18560 test images: 32.553879310344826 %\n",
      "Test Accuracy of the model on the 19200 test images: 32.578125 %\n",
      "Test Accuracy of the model on the 19840 test images: 32.57560483870968 %\n",
      "Test Accuracy of the model on the 20480 test images: 32.5341796875 %\n",
      "Test Accuracy of the model on the 21120 test images: 32.59943181818182 %\n",
      "Test Accuracy of the model on the 21760 test images: 32.58731617647059 %\n",
      "Test Accuracy of the model on the 22400 test images: 32.625 %\n",
      "Test Accuracy of the model on the 23040 test images: 32.682291666666664 %\n",
      "Test Accuracy of the model on the 23680 test images: 32.639358108108105 %\n",
      "Test Accuracy of the model on the 24320 test images: 32.65625 %\n",
      "Test Accuracy of the model on the 24960 test images: 32.56810897435897 %\n",
      "Test Accuracy of the model on the 25600 test images: 32.546875 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Load the pretrained model\n",
    "pretrained_model = resnet18(pretrained=True)\n",
    "pretrained_model.fc = nn.Identity()\n",
    "\n",
    "# Define the new classifier\n",
    "classifier = nn.Linear(512, 10)\n",
    "\n",
    "# Combine the pretrained model and the new classifier\n",
    "new_model = nn.Sequential(pretrained_model, classifier)\n",
    "\n",
    "# Freeze the weights of the pretrained model\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define the dataset and data loader\n",
    "transform = transforms.Compose([transforms.Resize((32, 32)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_dataset = datasets.SVHN(root='data', split='train', transform=transform, download=True)\n",
    "test_dataset = datasets.SVHN(root='data', split='test', transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the classifier's optimizer\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Train the classifier\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features = pretrained_model(images)\n",
    "        outputs = classifier(features)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, len(train_dataset)//train_loader.batch_size, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "new_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        features = pretrained_model(images)\n",
    "        outputs = classifier(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Test Accuracy of the model on the {} test images: {} %'.format(total, 100 * correct / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the third step you are performing transfer learining from MNIST to SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/svhn\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data/svhn\\test_32x32.mat\n",
      "Epoch [1/10], Step [100/1030], Loss: 3.3315\n",
      "Epoch [1/10], Step [200/1030], Loss: 3.1645\n",
      "Epoch [1/10], Step [300/1030], Loss: 3.0967\n",
      "Epoch [1/10], Step [400/1030], Loss: 3.2155\n",
      "Epoch [1/10], Step [500/1030], Loss: 3.2306\n",
      "Epoch [1/10], Step [600/1030], Loss: 3.6482\n",
      "Epoch [1/10], Step [700/1030], Loss: 3.1786\n",
      "Epoch [1/10], Step [800/1030], Loss: 3.1740\n",
      "Epoch [1/10], Step [900/1030], Loss: 3.4580\n",
      "Epoch [1/10], Step [1000/1030], Loss: 3.7884\n",
      "Epoch [2/10], Step [100/1030], Loss: 3.2257\n",
      "Epoch [2/10], Step [200/1030], Loss: 3.5579\n",
      "Epoch [2/10], Step [300/1030], Loss: 3.8133\n",
      "Epoch [2/10], Step [400/1030], Loss: 3.3742\n",
      "Epoch [2/10], Step [500/1030], Loss: 3.1836\n",
      "Epoch [2/10], Step [600/1030], Loss: 3.4071\n",
      "Epoch [2/10], Step [700/1030], Loss: 3.4735\n",
      "Epoch [2/10], Step [800/1030], Loss: 3.1275\n",
      "Epoch [2/10], Step [900/1030], Loss: 3.3214\n",
      "Epoch [2/10], Step [1000/1030], Loss: 2.8074\n",
      "Epoch [3/10], Step [100/1030], Loss: 3.2990\n",
      "Epoch [3/10], Step [200/1030], Loss: 3.4837\n",
      "Epoch [3/10], Step [300/1030], Loss: 3.2448\n",
      "Epoch [3/10], Step [400/1030], Loss: 3.4918\n",
      "Epoch [3/10], Step [500/1030], Loss: 3.4765\n",
      "Epoch [3/10], Step [600/1030], Loss: 3.7050\n",
      "Epoch [3/10], Step [700/1030], Loss: 3.6631\n",
      "Epoch [3/10], Step [800/1030], Loss: 3.1044\n",
      "Epoch [3/10], Step [900/1030], Loss: 3.3098\n",
      "Epoch [3/10], Step [1000/1030], Loss: 3.1779\n",
      "Epoch [4/10], Step [100/1030], Loss: 3.0445\n",
      "Epoch [4/10], Step [200/1030], Loss: 3.4946\n",
      "Epoch [4/10], Step [300/1030], Loss: 3.2325\n",
      "Epoch [4/10], Step [400/1030], Loss: 3.0926\n",
      "Epoch [4/10], Step [500/1030], Loss: 4.0264\n",
      "Epoch [4/10], Step [600/1030], Loss: 3.1141\n",
      "Epoch [4/10], Step [700/1030], Loss: 3.1569\n",
      "Epoch [4/10], Step [800/1030], Loss: 3.5757\n",
      "Epoch [4/10], Step [900/1030], Loss: 2.9651\n",
      "Epoch [4/10], Step [1000/1030], Loss: 3.2337\n",
      "Epoch [5/10], Step [100/1030], Loss: 3.0258\n",
      "Epoch [5/10], Step [200/1030], Loss: 3.1106\n",
      "Epoch [5/10], Step [300/1030], Loss: 3.0222\n",
      "Epoch [5/10], Step [400/1030], Loss: 3.2867\n",
      "Epoch [5/10], Step [500/1030], Loss: 3.2269\n",
      "Epoch [5/10], Step [600/1030], Loss: 3.3622\n",
      "Epoch [5/10], Step [700/1030], Loss: 3.3488\n",
      "Epoch [5/10], Step [800/1030], Loss: 2.8752\n",
      "Epoch [5/10], Step [900/1030], Loss: 3.0568\n",
      "Epoch [5/10], Step [1000/1030], Loss: 3.5586\n",
      "Epoch [6/10], Step [100/1030], Loss: 3.5280\n",
      "Epoch [6/10], Step [200/1030], Loss: 3.2466\n",
      "Epoch [6/10], Step [300/1030], Loss: 3.2718\n",
      "Epoch [6/10], Step [400/1030], Loss: 3.4815\n",
      "Epoch [6/10], Step [500/1030], Loss: 3.7008\n",
      "Epoch [6/10], Step [600/1030], Loss: 3.4648\n",
      "Epoch [6/10], Step [700/1030], Loss: 3.6776\n",
      "Epoch [6/10], Step [800/1030], Loss: 3.4369\n",
      "Epoch [6/10], Step [900/1030], Loss: 3.5684\n",
      "Epoch [6/10], Step [1000/1030], Loss: 2.9363\n",
      "Epoch [7/10], Step [100/1030], Loss: 4.0322\n",
      "Epoch [7/10], Step [200/1030], Loss: 3.1603\n",
      "Epoch [7/10], Step [300/1030], Loss: 3.6945\n",
      "Epoch [7/10], Step [400/1030], Loss: 3.4171\n",
      "Epoch [7/10], Step [500/1030], Loss: 3.1083\n",
      "Epoch [7/10], Step [600/1030], Loss: 2.8937\n",
      "Epoch [7/10], Step [700/1030], Loss: 2.9849\n",
      "Epoch [7/10], Step [800/1030], Loss: 3.6011\n",
      "Epoch [7/10], Step [900/1030], Loss: 3.0780\n",
      "Epoch [7/10], Step [1000/1030], Loss: 3.5022\n",
      "Epoch [8/10], Step [100/1030], Loss: 3.5730\n",
      "Epoch [8/10], Step [200/1030], Loss: 3.3974\n",
      "Epoch [8/10], Step [300/1030], Loss: 3.4384\n",
      "Epoch [8/10], Step [400/1030], Loss: 3.3214\n",
      "Epoch [8/10], Step [500/1030], Loss: 2.9611\n",
      "Epoch [8/10], Step [600/1030], Loss: 3.1765\n",
      "Epoch [8/10], Step [700/1030], Loss: 3.6452\n",
      "Epoch [8/10], Step [800/1030], Loss: 3.4177\n",
      "Epoch [8/10], Step [900/1030], Loss: 3.1132\n",
      "Epoch [8/10], Step [1000/1030], Loss: 3.9339\n",
      "Epoch [9/10], Step [100/1030], Loss: 3.6267\n",
      "Epoch [9/10], Step [200/1030], Loss: 3.5526\n",
      "Epoch [9/10], Step [300/1030], Loss: 3.2559\n",
      "Epoch [9/10], Step [400/1030], Loss: 3.6688\n",
      "Epoch [9/10], Step [500/1030], Loss: 3.1366\n",
      "Epoch [9/10], Step [600/1030], Loss: 3.4653\n",
      "Epoch [9/10], Step [700/1030], Loss: 3.4242\n",
      "Epoch [9/10], Step [800/1030], Loss: 3.4800\n",
      "Epoch [9/10], Step [900/1030], Loss: 3.5159\n",
      "Epoch [9/10], Step [1000/1030], Loss: 3.5057\n",
      "Epoch [10/10], Step [100/1030], Loss: 3.3869\n",
      "Epoch [10/10], Step [200/1030], Loss: 2.8456\n",
      "Epoch [10/10], Step [300/1030], Loss: 3.3870\n",
      "Epoch [10/10], Step [400/1030], Loss: 3.3853\n",
      "Epoch [10/10], Step [500/1030], Loss: 3.5791\n",
      "Epoch [10/10], Step [600/1030], Loss: 3.1099\n",
      "Epoch [10/10], Step [700/1030], Loss: 3.6693\n",
      "Epoch [10/10], Step [800/1030], Loss: 3.6358\n",
      "Epoch [10/10], Step [900/1030], Loss: 3.2027\n",
      "Epoch [10/10], Step [1000/1030], Loss: 3.3719\n",
      "Step [10/406], Test Accuracy: 19.06%\n",
      "Step [20/406], Test Accuracy: 18.28%\n",
      "Step [30/406], Test Accuracy: 19.06%\n",
      "Step [40/406], Test Accuracy: 19.61%\n",
      "Step [50/406], Test Accuracy: 19.38%\n",
      "Step [60/406], Test Accuracy: 19.24%\n",
      "Step [70/406], Test Accuracy: 19.51%\n",
      "Step [80/406], Test Accuracy: 19.53%\n",
      "Step [90/406], Test Accuracy: 19.29%\n",
      "Step [100/406], Test Accuracy: 19.42%\n",
      "Step [110/406], Test Accuracy: 19.40%\n",
      "Step [120/406], Test Accuracy: 19.31%\n",
      "Step [130/406], Test Accuracy: 19.33%\n",
      "Step [140/406], Test Accuracy: 19.43%\n",
      "Step [150/406], Test Accuracy: 19.49%\n",
      "Step [160/406], Test Accuracy: 19.59%\n",
      "Step [170/406], Test Accuracy: 19.60%\n",
      "Step [180/406], Test Accuracy: 19.52%\n",
      "Step [190/406], Test Accuracy: 19.46%\n",
      "Step [200/406], Test Accuracy: 19.37%\n",
      "Step [210/406], Test Accuracy: 19.47%\n",
      "Step [220/406], Test Accuracy: 19.44%\n",
      "Step [230/406], Test Accuracy: 19.47%\n",
      "Step [240/406], Test Accuracy: 19.48%\n",
      "Step [250/406], Test Accuracy: 19.50%\n",
      "Step [260/406], Test Accuracy: 19.51%\n",
      "Step [270/406], Test Accuracy: 19.58%\n",
      "Step [280/406], Test Accuracy: 19.59%\n",
      "Step [290/406], Test Accuracy: 19.51%\n",
      "Step [300/406], Test Accuracy: 19.51%\n",
      "Step [310/406], Test Accuracy: 19.48%\n",
      "Step [320/406], Test Accuracy: 19.43%\n",
      "Step [330/406], Test Accuracy: 19.47%\n",
      "Step [340/406], Test Accuracy: 19.43%\n",
      "Step [350/406], Test Accuracy: 19.51%\n",
      "Step [360/406], Test Accuracy: 19.57%\n",
      "Step [370/406], Test Accuracy: 19.53%\n",
      "Step [380/406], Test Accuracy: 19.50%\n",
      "Step [390/406], Test Accuracy: 19.57%\n",
      "Step [400/406], Test Accuracy: 19.50%\n",
      "Test Accuracy: 19.51%\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "torch.save(model.state_dict(), 'cnn-mnist.pt')\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "svhn_data = datasets.SVHN(root='./data/svhn', split='train', transform=preprocess, download=True)\n",
    "svhn_test_data = datasets.SVHN(root='./data/svhn', split='test', transform=preprocess, download=True)\n",
    "train_size = int(0.9*len(svhn_data))\n",
    "test_size = len(svhn_data) - train_size\n",
    "svhn_train_set, svhn_val_set = torch.utils.data.random_split(svhn_data, [train_size,test_size])\n",
    "\n",
    "svhn_train_loader = torch.utils.data.DataLoader(dataset=svhn_train_set, batch_size=64, shuffle=True)\n",
    "svhn_test_loader = torch.utils.data.DataLoader(dataset=svhn_test_data, batch_size=64, shuffle=False)\n",
    "svhn_val_loader = torch.utils.data.DataLoader(dataset=svhn_val_set, batch_size= 64, shuffle=False)\n",
    "\n",
    "pretrained_model = CNN()\n",
    "pretrained_model.load_state_dict(torch.load('cnn-mnist.pt'))\n",
    "optimizer = optim.Adam(pretrained_model.parameters(), lr=0.001)\n",
    "for params in pretrained_model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "pretrained_model.conv1 = nn.Sequential(\n",
    "    nn.Conv2d(3,1,1),\n",
    "    model.conv1\n",
    ")\n",
    "pretrained_model.last_linear = nn.Sequential(\n",
    "    nn.Linear(in_features=100, out_features=100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=100, out_features=10),\n",
    ")\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(svhn_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pretrained_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, len(svhn_train_set)//svhn_train_loader.batch_size, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "pretrained_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(svhn_test_loader):\n",
    "        outputs = pretrained_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Step [{}/{}], Test Accuracy: {:.2f}%'.format(i+1, len(svhn_test_data)//svhn_test_loader.batch_size, 100 * correct / total))\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print('Test Accuracy: {:.2f}%'.format(test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
