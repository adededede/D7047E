{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8102d1d8",
   "metadata": {},
   "source": [
    "Task 1\n",
    "\n",
    "We can train an RNN model to predict the next character given a sequence of characters. Such models are affectionately known as Char-RNNs.\n",
    "\n",
    "Train a Char-RNN model on the Tiny Shakespeare dataset. You can use the following repositories as a reference:\n",
    "\n",
    "• Char-RNN in PyTorch: https://github.com/spro/char-rnn.pytorch\n",
    "\n",
    "• Char-RNN in Tensorflow: https://github.com/karpathy/char-rnn\n",
    "\n",
    "Report the change in perplexity during training. Plot a graph to do so. Smoothen the curve if required. Plots that are not readable or are not self explanatory will not be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba7b6bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\olfia\\anaconda3\\envs\\env_pytorch\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Collecting zipp>=3.1.0\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Installing collected packages: zipp, importlib-resources, tqdm\n",
      "Successfully installed importlib-resources-5.4.0 tqdm-4.64.1 zipp-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3998b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1598f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.filename = 'filename'\n",
    "        self.model = \"gru\"\n",
    "        self.n_epochs = 500\n",
    "        self.print_every = 100\n",
    "        self.hidden_size = 100\n",
    "        self.n_layers = 2\n",
    "        self.learning_rate = 0.01\n",
    "        self.chunk_len = 200\n",
    "        self.batch_size = 100\n",
    "        self.shuffle = True\n",
    "        self.cuda = False\n",
    "\n",
    "args = Args()\n",
    "args.filename = 'shakespeare.txt'\n",
    "#An instance of the Args class is created = args. The filename attribute is then modified to 'shakespeare.txt'.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "\n",
    "if args.cuda:\n",
    "    print(\"Using CUDA\")\n",
    "    \n",
    "#If CUDA is enabled (args.cuda == True), the message \"Using CUDA\" is printed.\n",
    "    \n",
    "#The Args class defines a set of default configuration parameters for the model. It initializes several attributes such as filename ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c6ebb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def forward2(self, input, hidden):\n",
    "        encoded = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3cca62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Reading and un-unicode-encoding data\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "def read_file(filename):\n",
    "    file = unidecode.unidecode(open(filename).read())\n",
    "    return file, len(file)\n",
    "\n",
    "file, file_len = read_file(args.filename)\n",
    "# Turning a string into a tensor where each character in the string is represented as an index into the all_characters string. \n",
    "\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "    return tensor\n",
    "\n",
    "# Readable time elapsed\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51e708f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=False, tensor_fn=char_tensor, chars=all_characters, word = False):\n",
    "    predicted = prime_str\n",
    "#The temperature parameter controls the randomness of the sampling process.Lower values of temperature result in more predictable output \n",
    "    if word:\n",
    "      prime_str = prime_str.split()\n",
    "      #print(prime_str)\n",
    "\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    prime_input = Variable(tensor_fn(prime_str).unsqueeze(0))\n",
    "\n",
    "    if cuda:\n",
    "        hidden = hidden.cuda()\n",
    "        prime_input = prime_input.cuda()\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "        \n",
    "    inp = prime_input[:,-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
    "        if cuda:\n",
    "            inp = inp.to(device)\n",
    "\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8312a467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [02:06<08:35,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2m 6s (100 20%) 1.7867]\n",
      "Where chead for not frown, whilence ferfieR\n",
      "The hast\n",
      "Frither shall word me that be for the manent no p \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [04:19<07:11,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4m 19s (200 40%) 1.5540]\n",
      "Which a fair at thou onon;\n",
      "And what, which he have their father'd\n",
      "What do can dread like of the king m \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [06:34<04:35,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6m 33s (300 60%) 1.5264]\n",
      "Where is thy lord in thee to with your dull forget\n",
      "Face do thy chast you upon the read\n",
      "you deserved's  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 400/500 [08:52<02:17,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8m 52s (400 80%) 1.4393]\n",
      "Whild the die, sir: but as I though I will her need.\n",
      "\n",
      "LEONTES:\n",
      "Has under be made, kills the king been  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [11:11<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11m 11s (500 100%) 1.4331]\n",
      "Whill not; but you will I am the Jurether; but senate\n",
      "Their house and been till 'tis man, I them had n \n",
      "\n",
      "Saving...\n",
      "Saved as shakespeare.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\olfia\\anaconda3\\envs\\env_pytorch\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CharRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "from ctypes import c_char_p\n",
    "def random_training_set(args, file=file, file_len=file_len, tensor_fn=char_tensor):\n",
    "    inp = torch.LongTensor(args.batch_size, args.chunk_len)\n",
    "    target = torch.LongTensor(args.batch_size, args.chunk_len)\n",
    "    for bi in range(args.batch_size):\n",
    "        start_index = random.randint(0, file_len - args.chunk_len - 1)\n",
    "        end_index = start_index + args.chunk_len + 1\n",
    "        chunk = file[start_index:end_index]\n",
    "        inp[bi] = tensor_fn(chunk[:-1])\n",
    "        target[bi] = tensor_fn(chunk[1:])\n",
    "    inp = Variable(inp)\n",
    "    target = Variable(target)\n",
    "    if args.cuda:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    return inp, target\n",
    "\n",
    "# training of the model on a single batch of data. \n",
    "#The loss is computed for each character and the gradients are backpropagated through time.\n",
    "\n",
    "def train(args, inp, target, decoder, criterion, decoder_optimizer):\n",
    "    hidden = decoder.init_hidden(args.batch_size)\n",
    "    if args.cuda:\n",
    "        hidden = hidden.cuda()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(args.chunk_len):\n",
    "        output, hidden = decoder(inp[:,c], hidden)\n",
    "        loss += criterion(output.view(args.batch_size, -1), target[:,c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / args.chunk_len\n",
    "\n",
    "def save(args, decoder, save_filename):\n",
    "    save_filename = save_filename + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "    \n",
    "\n",
    "# Initialize models and start training\n",
    "\n",
    "decoder = CharRNN(\n",
    "    n_characters,\n",
    "    args.hidden_size,\n",
    "    n_characters,\n",
    "    model=args.model,\n",
    "    n_layers=args.n_layers,\n",
    ")\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.cuda:\n",
    "    decoder.cuda()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "args.chunk_len = 200\n",
    "\n",
    "try:\n",
    "    print(\"Training for %d epochs...\" % args.n_epochs)\n",
    "    for epoch in tqdm(range(1, args.n_epochs + 1)):\n",
    "    #for epoch in tqdm(range(1, 2)):\n",
    "        loss = train(args, *random_training_set(args, file, file_len, tensor_fn=char_tensor), decoder, criterion, decoder_optimizer)\n",
    "        loss_avg += loss\n",
    "        all_losses.append(math.exp(loss))\n",
    "\n",
    "        if epoch % args.print_every == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / args.n_epochs * 100, loss))\n",
    "            print(generate(decoder, 'Wh', 100, cuda=args.cuda), '\\n')\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    save(args, decoder, \"shakespeare\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Saving before quit...\")\n",
    "    save(args, decoder, \"shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c71d1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA1hUlEQVR4nO3dd3gVVfrA8e+b3gMJoQYIJRB6AqEISFeqqLu4FnTFstZdbD8VXQuuZd1F2V1dGzbsYkNYFQVFQKX3HiAQINQQSEhCes7vj5lcUiEh5Sa57+d58uTec6e85wbeOXPmzBkxxqCUUsp1uDk7AKWUUrVLE79SSrkYTfxKKeViNPErpZSL0cSvlFIuRhO/Ukq5GE38qkaJyAIRubG6l62rRGS6iHxYDdvZJiLDqh6RUqVp4leliEh6kZ8CEcks8n5yZbZljBlrjHmvupetDBEZZtcjXUTSRCRORG6q7v1UJ2NMN2PMEqieg4mIjBaRZXb9k0RkqYhMrJZgVb2jiV+VYowJKPwBDgCXFSn7qHA5EfFwXpSVdtiuTxDwMPCmiHStzAbqWX0dRGQS8DnwPhAONAOeAC67gG2JiGjeqOf0D6gqzG45J4rIwyJyFHhXRBqLyDd2K/KU/Tq8yDpLRORW+/UUEflVRF6wl90nImMvcNl2RVqwP4rIKxVpFRvL18ApoKuIuInINBGJF5FkEflMRELsfUSIiBGRW0TkALC4SNltInJYRI6IyAPn+M4GiMhyEUkRkU2F3TciMlBETohIa/t9L3uZKPt9goiMEpExwKPA1fYZyyYRuUpE1pXYzwMi8nUZ+xdgJvC0MeYtY0yqMabAGLPUGPMne5liZxRF6uhR5O/yrIj8BpwBHhWRtSX2c5+IzLdfe9t/twMickxEXhcR3/P9bVTt0cSvKqs5EAK0BW7D+jf0rv2+DZAJ/Pcc6/cH4oAmwD+Bt+3kVNllPwZWA6HAdOCGigRvJ/orgUbAFmAqcAUwFGiJdUB4pcRqQ4EuwOgiZcOBSOBSYJqIjCpjX62Ab4FnsL6z/wO+FJEwY8xy4A3gPTspfgA8ZozZWXQbxpjvgeeAOfYZVy9gPtBORLoUWfR6exsldQZaA1+c42upiBuw/t6BwMtAZxGJLPL5dVh/E4B/AJ2AaKAj0ArrDEPVEZr4VWUVAE8aY7KNMZnGmGRjzJfGmDPGmDTgWaxEWZ79xpg3jTH5wHtAC6yuhwovKyJtgL7AE8aYHGPMr1jJ8FxaikgKcAJ4ErjBGBMH3A781RiTaIzJxjqITCrRrTPdGJNhjMksUvaUXbYF68B3bRn7vB74zhjznd3KXgSsBcYVbhcIxjqAHab0AadMdpxz7O0jIt2ACOCbMhYPtX8fqci2z2G2MWabMSbPGJMKzMOus30AiALm2wfmPwH3GWNO2v8mngOuqeL+VTXSxK8qK8kYk1X4RkT8ROQNEdkvIqeBZUAjEXEvZ/2jhS+MMWfslwGVXLYlcLJIGcDB88R92BjTyBgTYoyJNsZ8ape3Beba3SwpwA4gn+IHo7K2XbRsvx1TSW2Bqwq3bW9/MNYBDGNMLjAb6A68aCo3Y+J7wHV2or0B+Mw+IJSUbP9uUYltl6Xkd/AxZw921wFf23+PMMAPWFekzt/b5aqO0MSvKqtkcnoAqzuhvzEmCBhil5fXfVMdjgAhIuJXpKz1BW7rIDDWPigU/vgYYw4VWaashFx0f22wWuxlbfuDEtv2N8Y8D46uoCexzhheFBHvcmIstX9jzEogB7gYK/GW1c0DVlfZQeD35XwOkIGVrAs1r0AMC4EmIhKNdQAo7OY5gdXd161InYPtC+uqjtDEr6oqEOs/eop9UfTJmt6hMWY/VpfJdBHxEpGLuIARKrbXgWdFpC2AiISJyOUVWO9x+2ynG3ATVtdLSR8Cl4k1lNJdRHzEukAebrfUZwNvA7dgHcyeLmdfx4AIKT2a5n2s6yl5dndXKfZZxP12vDeJSJB9nWOwiMyyF9sIDBGRNiISDDxyvsobY/KwrhvMwLp+scguLwDeBP4lIk3BOsCJyOjytqVqnyZ+VVX/BnyxWnorsU7ra8Nk4CKsroxnsBJvWV0d5/MfrOsDC0UkDasO/Suw3lJgD/AT8IIxZmHJBYwxB4HLsUblJGG1vB/E+n83Fas76XE7Od8E3CQiF5exr8/t38kisr5I+QdY3UTltfYL4/gCuBq4GevM5BjWdzbP/nwR1ve3GVhH2dcKyvIxMAr43D4QFHoY67tZaXf//Yh1VqjqCNEHsaiGQETmADuNMTV6xiEiEcA+wLNEsqt19mig40BvY8xuZ8ai6hdt8at6SUT6ikgHu9tiDFbL+msnh1Xb7gTWaNJXlVVjdyKKyDvABOC4Maa7XRaCdUoZASQAfzDGnKqpGFSD1hz4Cmu4YiJwpzFmg3NDqj0ikoB1Af0K50ai6qMa6+oRkSFAOvB+kcT/T6xheM+LyDSgsTHm4RoJQCmlVJlqtI/f7g/9pkjijwOGGWOOiEgLYIkxRi/6KKVULartSaeaGWOOANjJv2l5C4rIbVi3iOPv798nKiqqlkI863haNsdOZ9G9ZTDlTiqglFJ11Lp1604YY0rdPFdnZxs0xswCZgHExsaatWvXnmeN6jdnzQEe/nIL8x4eTnhjv/OvoJRSdYiI7C+rvLZH9Ryzu3iwfx+v5f1XStNAHwBGvLCUggId9qqUahhqO/HPBwqfsHQj9g0kdVWTAOsO+pz8AvYkpTs5GqWUqh41lvhF5BNgBdb0rYkicgvwPHCJiOwGLrHf11kRTc5272w4oKNOlVINQ4318RtjypqmFmBkTe2zugX6eLLv7+OIeXoR6/encHXfNs4OSaly5ebmkpiYSFZW1vkXVg2Kj48P4eHheHp6Vmj5Ontxt64QEWJaN2K9tvhVHZeYmEhgYCARERGU/2wb1dAYY0hOTiYxMZF27dpVaB2dsqECYto0ZvfxdFIzc50dilLlysrKIjQ0VJO+ixERQkNDK3Wmp4m/AqJbNwJg2+FU5wai1Hlo0ndNlf27a+KvgE7NAgHYdTTNyZEopVTVaeKvgGZB1rDO6f/bzryNh86ztFKuJzk5mejoaKKjo2nevDmtWrVyvM/JyTnnumvXrmXq1Knn3cfAgQOrJdYlS5YQHBxMTEwMXbp04amnnqqW7SYkJNC9e/cLWnf+/Pk8/7w1yPHrr79m+/bt1RJTefTibgUUPY1atP0Yl0e3cmI0StU9oaGhbNy4EYDp06cTEBDA//3f/zk+z8vLw8Oj7HQTGxtLbGzsefexfPnyaokV4OKLL+abb74hIyOD6OhoJkyYQJ8+fc673rnqURUTJ05k4sSJgJX4J0yYQNeuXat9P4W0xV9Bz1xhHckb+VVsuJRSrm7KlCncf//9DB8+nIcffpjVq1czcOBAYmJiGDhwIHFxcYDVAp8wYQJgHTRuvvlmhg0bRvv27XnppZcc2wsICHAsP2zYMCZNmkRUVBSTJ0+mcLLJ7777jqioKAYPHszUqVMd2y2Pv78/ffr0IT4+nvj4eMaMGUOfPn24+OKL2blzZ5n1mD59OjfccAMjRowgMjKSN998s9R28/PzefDBB+nbty89e/bkjTfeAGDmzJncfPPNAGzZsoXu3btz5swZZs+ezZ///GeWL1/O/PnzefDBB4mOjiY+Pp7evXs7trt79+4KHaDOR1v8FXT9gLa8+9s+Tmac+7RVqbrgqf9tY/vh09W6za4tg3jysm6VWmfXrl38+OOPuLu7c/r0aZYtW4aHhwc//vgjjz76KF9++WWpdXbu3MnPP/9MWloanTt35s477yw1Pn3Dhg1s27aNli1bMmjQIH777TdiY2O5/fbbWbZsGe3atePaa8u7leis5ORkVq5cyeOPP85tt93G66+/TmRkJKtWreKuu+5i8eLFpeoxffp0Nm/ezMqVK8nIyCAmJobx48cX2+7bb79NcHAwa9asITs7m0GDBnHppZdy7733MmzYMObOncuzzz7LG2+8gZ/f2RtFBw4cyMSJE5kwYQKTJk0CIDg4mI0bNxIdHc27777LlClTKvU3KIsm/koI9fcmOV0Tv1IVddVVV+Hu7g5AamoqN954I7t370ZEyM0te3j0+PHj8fb2xtvbm6ZNm3Ls2DHCw8OLLdOvXz9HWXR0NAkJCQQEBNC+fXvHWPZrr72WWbNmldo+wC+//EJMTAxubm5MmzaNtm3bsnz5cq666irHMtnZZx/hXLQeAJdffjm+vr74+voyfPhwVq9eTXR0tOPzhQsXsnnzZr744gtH3Xfv3k27du2YPXs2PXv25Pbbb2fQoEHn/Q5vvfVW3n33XWbOnMmcOXNYvXr1edc5H038lRDi70W8ztmj6oHKtsxrir+/v+P1448/zvDhw5k7dy4JCQkMGzaszHW8vb0dr93d3cnLK/1o47KWqcyzRQr7+AudPn2aRo0aOa5TnKseUHr4ZMn3xhhefvllRo8eXWpbu3fvJiAggMOHD1co1t///vc89dRTjBgxgj59+hAaGlqh9c5F+/grISTAi70nMsjMyXd2KErVO6mpqbRqZQ2MmD17drVvPyoqir1795KQkADAnDlzKrxuUFAQ7dq14/PPPwesxL1p06Zyl583bx5ZWVkkJyezZMkS+vbtW+zz0aNH89prrznOanbt2kVGRgapqancc889LFu2jOTkZMcZQVGBgYGkpZ0dOu7j48Po0aO58847uemmmypcp3PRxF8Jvp7u5BcY/vr1FmeHolS989BDD/HII48waNAg8vOrv/Hk6+vLq6++ypgxYxg8eDDNmjUjODi4wut/9NFHvP322/Tq1Ytu3boxb175kwf369eP8ePHM2DAAB5//HFatmxZ7PNbb72Vrl270rt3b7p3787tt99OXl4e9913H3fddRedOnXi7bffZtq0aRw/Xnx2+muuuYYZM2YQExNDfHw8AJMnT0ZEuPTSSyvxjZSvRh+9WF2c9SCWkpbEHWfKu2to1ciX36aNcHY4ShWzY8cOunTp4uwwnCo9PZ2AgACMMdx9991ERkZy3333Ves+yhquWtNeeOEFUlNTefrpp8tdpqy/v4isM8aUGiurffyVMKxzU8Z0a65z8ytVR7355pu899575OTkEBMTw+233+7skKrsyiuvJD4+3jHCqDpo4q+ksEBvVu5LdnYYSqky3HfffdXewi9p+vTpNbr9kubOnVvt29Q+/kpqEuBNyplccvMLnB2KUqXUh65bVf0q+3fXxF9JTQK9AHQ8v6pzfHx8SE5O1uTvYgrn4/fx8anwOtrVU0mFz+E9kZ5N8+CKf9FK1bTw8HASExNJSkpydiiqlhU+gauiNPFXUstgXwD2ncige6uKDxVTqqZ5enpW+AlMyrVpV08lRbUIxM/LndX7TpKeXfqOQqWUqus08VeSp7sbsREhfLByPzF/W8juY/pwFqVU/aKJ/wLcNyqSQG8PcvMNry2Nd3Y4SilVKZr4L0BMm8ZseWo0AzuEciD5jLPDUUqpStHEXwWN/bw4eUaHdSql6hdN/FXQ2N+TU/pgFqVUPaOJvwoa+3mRmplLfoHeMKOUqj808VdBYz8vCgycziz7SUJKKVUXaeKvghB/a/oG7edXStUnmviroJGf9QBo7edXStUnmviroHDenji9iUspVY9o4q+CLi2C6NW6ETMX7iJFu3uUUvWEJv4qcHcT/n5lD1Iyc3l96V5nh6OUUhWiib+KurYMYkhkE/636bDOg66Uqhc08VeD8T1bciglk02Jqc4ORSmlzksTfzW4pGszvNzd+GbTYWeHopRS5+WUxC8i94nINhHZKiKfiEi9fpRVsK8nAzuG8nPccWeHopRS51XriV9EWgFTgVhjTHfAHbimtuOobn0jQohPytAx/UqpOs9ZXT0egK+IeAB+QL3vI4lt2xiAdftPOTkSpZQ6t1pP/MaYQ8ALwAHgCJBqjFlYcjkRuU1E1orI2vrw8OherRvh6S6s3X+KrYdSWbU32dkhKaVUmZzR1dMYuBxoB7QE/EXk+pLLGWNmGWNijTGxYWFhtR1mpfl4utOtZTDr9p9kwsu/cvWslc4OSSmlyuSMrp5RwD5jTJIxJhf4ChjohDiqXZ+2jYsN6czMyXdiNEopVTZnJP4DwAAR8RMRAUYCO5wQR7WLbduYnLwCx/s9x9OdGI1SSpXNGX38q4AvgPXAFjuGWbUdR03oE9G42PudR087KRKllCqfU0b1GGOeNMZEGWO6G2NuMMZkOyOO6tY00Advj7Nf6ZJddf+itFLK9eidu9WsXRN/AJoEePHt5iNMn7+N7Dzt61dK1R2a+KvZiKimANw7qhMAs5cnsGzXCWeGpJRSxWjir2b3X9KJt/4Yy+T+bZh7lzVYSS/yKqXqEg9nB9DQeLi7MaprMwBi2jSmaaA38Uma+JVSdYe2+GtYh7AAvliXyMaDKc4ORSmlAE38Ne7qvq0B+GHbUSdHopRSFk38NeyKmFa0auTLvqQMZvywU5/Nq5RyOu3jrwUtG/nw/bajsA0ysvOZPrGbs0NSSrkwbfHXgpaNfB2vN2hfv1LKyTTx1wJ3N3G83nQwhYMnzzgxGqWUq9PEXwuGdbZu6pp1Qx8ALv7nz5zUJ3UppZxE+/hrwcReLRnTrTleHm70iwhhdcJJ9hxPp1+7EGeHppRyQdriryVe9uRtz/2uBwCHUrS7RynlHJr4a1l4Y+tC76FTmU6ORCnlqjTx1zIfT3eaBHix78QZjDHODkcp5YI08TtBmxA/vlyfyNAZS4hPSufnncfJyM5zdlhKKRehid8J/vH7nvxlREeS0rK5+6P13DR7Dc8v2OnssJRSLkITvxNENgvkgUs7M6B9CDuPpgGQnNEgHkKmlKoHNPE7UY/wRo7XTQN9nBeIUsqlaOJ3osEdmzhez16ewDu/7mPGDzt1/n6lVI3SxO9E/dqFMPumvo73f/tmO6/8HM9f525xYlRKqYZOE7+TDevclA5h/sXKgnw8nRSNUsoVaOKvA7JyC4q99/Nyd1IkSilXoIm/DsjOyy/2/nBKFjl5BeUsrZRSVaOJvw4obPFf07c1F7UPZXXCSSa/tdLJUSmlGipN/HWAp7s1X/9fx3ehsb/Vv78m4RS5+drqV0pVP038dcBHtw7gnpGRBPp4cuz02Ru5Lvr7YidGpZRqqHQ+/jqga8sgurYMAqzZO9ftPwXAifRsDp48g5+XO6EB3s4MUSnVgEh9mCEyNjbWrF271tlh1Iq0rFy2HjpNQnIGj3xljef38XRj05OX4u2ho32UUhUnIuuMMbEly7Wrp44J9PHkog6htG9ydmx/Vm4Bi7Yfc2JUSqmGRBN/HdU29Gzi93QXth467cRolFINiSb+OqppoDcXRzbh1cm96dg0kB1HNPErpaqHJv46ys1N+OCW/ozr0YJuLYNYuiuJUTOXkpqZy7yNh9h5VA8ESqkLo4m/HrhzWAcA9hxPZ9ayeO75dCP3frrRuUEppeotpyR+EWkkIl+IyE4R2SEiFzkjjvqiQ1gA8/88CIBXfo4HrIPAfxfv5nCKPrRdKVU5zmrx/wf43hgTBfQCdjgpjnqj6Dj+bi2DyCswvLBwFwOfX8zy+BMkpWXz2ZqDToxQKVVf1PoNXCISBAwBpgAYY3KAnNqOo74J9fdyvH5wdGemvLvG8f66N1cRFuhNUlo2/duHFBsRpJRSJVWoxS8iIdW4z/ZAEvCuiGwQkbdEpFSmEpHbRGStiKxNSkqqxt3XTz6eZ2/eimwWyKL7hhT7PCnNmuph/Eu/cvx0Vq3GppSqXyra1bNKRD4XkXEiIlXcpwfQG3jNGBMDZADTSi5kjJlljIk1xsSGhYVVcZcNS4sgHyKbBRY7CyiUnp3HHO3yUUqdQ0UTfydgFnADsEdEnhORThe4z0Qg0Rizyn7/BdaBQFWQm5t17O3XzjoRmzY2ituHtnd8nldQ96fhUEo5T6Xn6hGR4cCHgD+wCZhmjFlRyW38AtxqjIkTkemAvzHmwfKWd6W5es5l9b6T5BUUMLCD9ZD2jOw84pPS6RneCIAfth3l9g/WAfDxrf1pHeLH9iOnGd2tubNCVko5UXlz9VTo4q6IhALXY7X4jwF/AeYD0cDnQLtKxvMX4CMR8QL2AjdVcn2XVNjCL+Tv7eFI+gCjuzUn0MeDtKw8rntrlaN81zNj8fLQWzaUUpaKjupZAXwAXGGMSSxSvlZEXq/sTo0xG4FSRyFVdQHeVuIv6nBKJhFNdKSPUspS0WbgY8aYp4smfRG5CsAY848aiUxdkPdv7sfr1/emkZ+no2zYC0t46ItNTHl3Na/8vAeAA8lnGPT8YvYcT3NWqEopJ6loi38a8FmJskewunlUHRLZLJDIZoE0D/Zl19E0HvpyMwCfrbWO2Uvikri2XxsWbD3CoZRM1h9IoUNYAFUfrKWUqi/OeXFXRMYC44A/AHOKfBQEdDXG9KvZ8Cx6cffCFBQY2j/6Xany7q2CCPLxZHl8Ms2CvMnIzmfpg8M4lJJJ68Z+NC5jmKhSqv650AexHAbWAlnAuiI/84HR1R2kql5ubsKHt/Tnl4eG88eL2gLw6Lgoth46zfL4ZACOnc4mPTuPX/ecYOJ/f2PiK786M2SlVC2o0HBOEfEwxuSdd8Eaoi3+qssvMKRl5eLj6U7U49+X+vyi9qGs2GsdDP46rgt/GtK+1DJKqfrlglr8IlLYr79BRDaX/KmRSFWNcHcTGvl54ePpzrtT+jKwQygB3tYlnpFRTR1JH+BfP+4iKzefr9YnkpNX4KyQlVI15HwXd++xf0+o6UBU7Rke1ZThUU2JT0on8VQmzYK8+WnncQCmjozkpZ9289X6Qzw6dwu/7D7Bv66Odm7ASqlqdc7Eb4w5Yr/0N8ZsL/qZiAwD9tdMWKo2dAgLoENYAACvTu7NpsQUerdpBMCSOOtAMHfDIZ65ojv+3h6cycnDx8PdMWWEUqp+qug4/s9E5GGx+IrIy8DfazIwVbvG9WjBI2O70LVFEACL7TMAgE9WHyDlTA5dn/iBlxbvdlaISqlqUtHE3x9oDSwH1mCN9hlUU0Ep5wkL9CbE34u8AkP7MH96hQfzzLc7iH3mRwA+WHH2JC83/2z/v14LUKr+qGjizwUyAV/AB9hnjNH/6Q2QiNClRSAArRr58sJVvfDycHPM+JmckUOnxxbw3Hc7iPzrAr7bcoRnvtlOp8cWsOuY3gWsVH1Q0cS/Bivx9wUGA9eKyBc1FpVyqm4tgwGICPUnslkgGx6/hL+M6OiYJC4nr4BZy/YCcNdH63nr130AbDyQ4thGUlq24zqBUqpuqeiUDbcYYwoH0h8FLheRG2ooJuVkdwztQHTrRgzpZD0Ax9/bgwcu7Ux8UjojX1xa7nqZufkYY8jKLaDvs1bX0C8PDad1iF+txK2UqpiKtvjXicj1IvIEgIi0AeJqLizlTCH+Xozr0cIxzr9Qh7AAnruyh+PJX/eOiuTRcVGOzxOSM7j6jZV0eeLsDWKbE1PZeiiV5PTscve3YMsRUs7oY5eVqi0VvXP3NaAAGGGM6SIijYGFxpi+NR0g6J27dU1Gdh6nzuQQ3tiPUxk5xDy9qNQy3VoGsetYGuN7tODrjYcBmPmHXoyMakawPXPo5LdW0qNVI15fGk/3VkF885eLa7UeSjV0VXoQC9DfGNNbRDYAGGNO2Q9RUS7I39sDf/tsoKwJ3b66ayC9whtx0+w1jqQPcP9nm2gT4sc7U2I5kZ7Db3uS+W2Pdcfw1kOnyc7Lx9vDvdT2lFLVq6KJP1dE3AEDICJhWGcASrHusVEkZ+Tw5bpEftl9gu4tg3F3E16b3Jt/fr+TExk5fLvZuhcwIzuPUTOXlbmdlXtPMtS+rgA4DgT5BQZ3N8EYU2z66ONpWTTx93b6DWWLth+jZSMfx0Vxpeq6inb1TAauxnoo+nvAJKyHs9TKfPza1VP/XTtrJR7uwvgeLZj21RYuah9KIz9PFmw9CoCvpzuN/TyZPKAttw1pjzEwdMbP+Hm5E5+UwR1DOzB7+T6+uGMg3VsFs3jnMW6evZa/juvCJV2bcfDUGS6OPHvQSDmTQ/TfFvGvq3sREerPn95fxzNXdGdM9+p//nDEtG8BSHh+fLVvW6mqqFJXjzHmIxFZB4wEBOsRjDuqOUbVgH1y2wAAjDEMjmxCq0a+xCdlsPFgCkMiw3BzEz5ZfYAZP8RxIj2bvhEhHEnNcqz/+tJ4ACa8/CuvX9+Hz9ceBOC1pfE8+531TzHumTGOrqL9yWcAuG/OJiKbBnAiPZt5Gw8xpntzftx+jJ7hwTQN8qlyvQoKzt9wUqquOWfiF5GiT/c+DnxS9DNjzMmaCkw1TCJCeGNreGfHpgGseGQkAMnp2Xi6C/M3Hebd3xJ497cEvNzdyMkv3aP4+dqDJCRnAHAy4+xooM6Pfc+Gxy9h2e4kjp0+e9DYfTwdgK2HUzmVkcOt76+lWZA3qx4dVeX6nM7KLbM8N7+AuKNpdG+l3T+q7jlfi38dVr9+WZ2oBtBJ21W1CA3w5m+Xd6dj0wCemLcNN4GXro1mQPtQ1iSc4sl5W5kyKIKjqdm8vyKBvAJDbNvGrN1/qth2HvlqC99vO1pq+0M6hbFsVxI/2zeVHTudzbWzVjKyS1NGdWlW7GH0+5MzmLvhEDcPbkeQjyeZOfm4uVHmhefkjLKHob7wQxxvLNvLj/cPpWPTgKp8NUpVu/PNztmutgJRCuD6/m0Z0615sW6YS7o245KuzQBYvPMY7/xm3Sk8qU84Y3u04OlvttM8yIejp7PKTPozJvWksZ8Xy3YlMXfDIUf5ir3JrNibzDPf7mBs9+ZM7t+WwZFN+NeiXXy98TD//nE3//vzYC5/5Vc6hAWw6P6hpbZd9Ixjf3IGbUOtA8iGgykAHErJLDfx5+YXMGfNQf4Q2xovj4reUqNU1VX4X5uI/E5EZorIiyJyRQ3GpFyYm5ucs++9T5uzvY+RzQK4pm9r/nhRW76752L8vKwWeYvgs+t7e7hxZUwrIppY3Uu/7D7hmHq6qAVbj/Kn99eSkZ1X7Cxi7oZDFBiru+hA8hn2JlndRnuOp3EqI4fk9LOJf+iMJaRm5jr2C3AkJbPcuszdcIjHvt7Km7/sLXcZpWpChRK/iLwK3AFsAbYCd4jIKzUZmFJlCfbzZFSXZlw/oA292zTG39uDv13enRB/L356YCirHh3puG4AEPfMWDzc3RzXFQCu7B3Om38sPtChT9vGZObmc/9nG0k8lcnzv+vBqC5Nmb18n2OZITN+ZsSLS7nn0w2MmrmMITN+5uDJM8W2897yhGLvD54q/rkxhsKRdIftg8KMH+L4ZvPZ+x2S07Pp8eQPrCzyVLSSsnLzycrNL/OzTQdTHNsG2HoolQVbjpS5rHJNFR3HPxTobux/sSLyHtZBQKla99aNpUanAdAi2NfxetrYqGKJ0cfzbP/8lTGtCPD2YMrACGbbifqPF7Vlf/IZfth2jNi2jZnUJ5xmQT78uKP0RHPz7JvS0rLy+GTNgWKffbzqAHuOp/PL7hMAzFmTyJ3DOjLj+53EJ2Ww70QGV/dtzdSRkcQdPTub6fdbjzK+RwvyCwxrEk6Slp3HNbNW8smfBnBRh9Bi+9ifnMH4l36laaA3Pz0wFBEhOy8fDzc3BLj8ld8I8PZgYIdQbh/ant+/tgKo3HDTrNx83ES0C6qBqmjijwPacPaJW60BfeauqrPuGNqhVNl7N/cj2NfTMQfR9InduHt4R975bR9ju7egTYgfr/y8hycv64aHu1uxm8lK+uqugfzu1eXsTcpwlIU39iXxVCbzN51tvZ9Iz+a1JXt4r8hzDN5bnkB2Xj4/7jjmeMi9iPCvH3fz1i97mToy0rHstW+uZNWjI3li3lb6RoRw68XtmbfxMOnZeaRn57H3RAYdwgLo/Jg1P9KVMa0ASM/OY+H2Y8W6rVLP5DqmyzifLk98T6emgfxw35BzLrftcCofrtzPM1f04HBKJmGB3sUOsqpuqugNXEuxpmRebRf1BVYAZwCMMRNrKkDQG7iU8xxOyeTTNQf5ZXcSUwZGcM+nGwGr9Tz2P7+w48hpBrQPYeXekzxwSSd2HkvDz9Odz9clclWfcBZuP+bo9y9pYq+WPHFZV+76cD15BQWsLzKtdXnm3DaAP72/Fi8PN06k53BtvzZk5eYXu2hdni/vvIgfdxxn2+HTTBnYlhFR1gXz3PwCHpu7lZsHt6Nzc+tZDOe6Ke3LdYm8uDCOpQ8NZ/S/l7E3KYOPbu3PTbPXkJNXwLX92vD33/U4bzyq5pV3A1dFE3/p4QxFGGPKn6u3GmjiV3XFZ2sPIsBVsa05kHyGtOxc2oT48eLCXTw4urNjDqOEExk0C/Lhild+I+5YGpf1aklEqB8D2ocy+a1VtAz2YcmDw/HycOPuj9c7prSoqFcn92b6/G0cTyt/1tOSxvdowbdF+vpbh/jy9yt74uPpxqTXV9CrdSMeG9+FZbuSeHnxHgBuGdyOsd2bExrgTUSoH7/uOcENb1vtv0EdQx1zLXVuFkhckQfxLH1wGKEB3o6zq9TMXHLyCvB0F15dEs+fLm7PzEVxNAnw5tbB7fl2yxGuig3H0/38XUvzNh7C28ONMd1bVLjuruqCE789R88Pxpiq3+1ygTTxq/rqgxUJvL9iPx//aQBhgd6AdfE2r8DQzB699H+fb+KLdYkAfH/vxZxMz+G6t1YB8M1fBjPh5V+LbfP6AW14+vLu/OGNFaxJKH4fQ1WJQHkp4ZXrenP3x+srvK3R3Zrx8JgoFmw9ysxFu2gW6E3XlsH8uONYseWmX9aV6f/bznX92/DclaXPFNKyctl++DT921vXOkqejXy7+QhvLIvng1v6E+xrdWU9v2AnK/YmM+/uC3tCbHp2HgXGEORTsa6xuqq8xH/ew6sxJh84IyJ6C6JSlXTDRREsun+oI+mDdbNasyJDVgvvMn77xliimgcxsGMTfrx/CM//rgfdWwUzbWxUsW0+eVk3RIRGfsVnRn3lut40C/LmrT/G8u3UwXx62wA+vrU/3997MYM6Wknz1sHt+ENseLnx9m8XUqqsV7j1X3/aV+e+rFdyrrxlu04w9j+/MOOHOPILDIdTs0olfYC4Y9YQ2Y9XHeCP76xmzpoD5OQVcCglk9z8Aq56fQVXz1rJN5sPk1fkTu68/AKycvN585e9bE5M5aWfdjsu6L++NJ5NB1Mcy//hjRXM3WAdXI0xbDqYwsq9yWRk57Em4STzNh7iwc838dJPuwG4/YO19Jy+kO2HT5eKNys3n22HUwHrzO7Zb7cXe+b0wm1Hufvj9VSkNwWsUVexzyzieJG7zWtaRS/uZgFbRGQR4LiaZYyZWiNRKeVCHh4ThbfHbgZ1bOIo69g0kI5Nrf72O4Z2oGd4MNe9uYrLerV0dIc8NbEb7cP86RgWwE87jjO+ZwvG9yy7++Nff4jmm81HuOGitjzw2aZin03qE+4447hnZCca+SY4boS7PLol/7kmhsH/WEziqUwGtA8hunVjQvw9ee67nY5tPHdlD+ZtPMSqfWdnccksMdz0hgFtScvK5aExUQx8frGj/Osi1yeW7Uri191JfLL6IBsPpvDn4R3ZaY9+en/5fqLsaxAA/9t8mFnL9rHjiJWc3/51Hx+t2s/lvVo5ljlw8gxrE06xet9JVu87yZUx4czdcIj77e+grLu/bxvS3tGFtWDrEfuZ0wV0ahqIm5vw6Fdb+GrDIUZ3a8aJ9BzW7T9F5+ZBTOoTzutL43l+gfW93DGkAz3Cgzmcksl9czYyqU84V8W25tvNR9hw4BRRLYIQYOmuJE6k57BkVxJ/iG1d5t+vulW0j//GssqNMe9Ve0Rl0K4e5eoKCgzrDpyid5vGuFdxGuq4o2k89vUWIpsFWsNPnx1L3LE05q4/xKPjupCTX8CKvcl8s+kID4/pTNMgH9o/8i0FBt6d0pfhUU0BOJqaxZAZPzNjUk8uj25FwokMXl2yhyYB3vh6uvPiol3F9lv0QvGGA6eY8UMcWw+lcjorj4hQPxKSzzB1ZCSzf9vH6aw8x7I9w4Pp3aYxn645QFZu5WaDH9Y5jCVxSY73707py6xle1mxN5lAbw/SsvNKrVN4sb4kN4Fdz4yl02MLKDo3n5tY8059f88Qej61kHR7m1NHdOT+Szvz38W7eWGh9V2M79GC5fEnOHWm9AX/Jy/ryk2Dzk6WMPu3fYQF+pR7MK+IKl3ctTfgC7QxxtT6Ixc18StV/Ywx5BWYCl1QfWNpPDMX7WLrU6MrtHxmTj6frT1Iq0a+3Pq+9X+3rBFCH6zcz+Nfb2VUl6b8c1IvQvy9+H7rEV5dEs+uY2lk5RYwdURH2ocFcO+cjQB4ebjx0jUx/OWT9eTmX9jsqH8e3pH7L+nEoH8sdswC2ys8mE2JqY59XBHdks/WJhZb74s7LmLS6yuKlRVeo3jp2hjum7ORO4a2Z0lcEiH+XrwzpS9XvPIb28roMirpdzGtCG/sy4AOocS2DaHTYwsAiH9u3AUf7Ks6qucy4AXAyxjTTkSigb/V9DDOQpr4laq/IqZ9y5UxrfjX1dGlPjPG8OYve7k4MowuLYKKfbYk7jhT3l3D3LsG0jrEj6mfbGDa2Ch6tApGRPg57jh3fbie/14XgzE4DjD3jIzkjqEd+HrjIY6kZPKSPUKpUNNAb1Y+MhI3N+FIaiYvL95DTl4BUwZGMPmtVaRm5rL2sVH4ebmzP/kM/l4ejJq5lJz8AsZ2b86CrUe5PLol/dqF0L9dCBGh/ox76Rd22dcq/nNNNCvik/l0zUHHPoueebgJnGs277BAb168qhd/fMcaPfX+zf0Yco57Ss6lqol/HTACWGKMibHLthhjamWwriZ+peqvggKDCMWenlZRSWnZxS6Mn8vprNwyR+GsSTjJVUVa6eN6NOfVyX3K3Ma+ExkcPHmmVKLNys0n6nHrJrlgX0/WP35JsVb45sQUJv73NwAW3HMxq/YmM/1/2x2ff3nnQH7/2nJEYMW0kaRm5vLJ6gM8Mi6KLo9/7zgQhAV6k1RkiO7dwztw48AImgZe2LMjqpr4Vxlj+ovIhiKJf7MxpucFRYNjmOha4JAxZsK5ltXEr5Sqil3H0njs6630iwjhpkERhAZU7GBS1J0frmPr4VSmjelSZr/7oZRMth8+zSVdm3HsdBb/WLCTu4Z34GhqNoM6hjJ7eQLDOzctNgU44Bj9s2JvMn3aNmbBlqM8OncLdw7twF+K3MV9Iaqa+N8GfgKmAb8HpgKexpg7qhDQ/UAsEKSJXymlzsrMycfXq+pTX1zwOH7bX4BuQDbwMZAK3FuFYMKB8cBbF7oNpZRqqKoj6Z/L+R696IM1HXNHrNk4LzLGlB7/VHn/Bh4CAstbQERuA24DaNOmTTXsUimlFJy/xf8eVnfMFmAs1sieKhGRCcBxY8y6cy1njJlljIk1xsSGhV3YFW2llFKlne/O3a6FI3fsfv7V51m+IgYBE0VkHOADBInIh8aY66th20oppc7jfC1+x+1l1dTFgzHmEWNMuDEmArgGWKxJXymlas/5Wvy9RKTwljMBfO33AhhjTFD5qyqllKqLzpn4jTE1emnZGLMEWFKT+1BKKVWcPlBTKaVcjCZ+pZRyMZr4lVLKxWjiV0opF6OJXymlXIwmfqWUcjGa+JVSysVo4ldKKRejiV8ppVyMJn6llHIxmviVUsrFaOJXSikXo4lfKaVcjCZ+pZRyMZr4lVLKxWjiV0opF6OJXymlXIwmfqWUcjGa+JVSysVo4ldKKRejiV8ppVyMJn6llHIxmviVUsrFaOJXSikXo4lfKaVcjCZ+pZRyMZr4lVLKxWjiV0opF6OJXymlXIwmfqWUcjGa+JVSysVo4ldKKRejiV8ppVyMJn6llHIxtZ74RaS1iPwsIjtEZJuI3FPbMSillCvzcMI+84AHjDHrRSQQWCcii4wx250Qi1JKuZxab/EbY44YY9bbr9OAHUCr2o5DKaVclVP7+EUkAogBVpXx2W0islZE1iYlJdV6bEop1VA5LfGLSADwJXCvMeZ0yc+NMbOMMbHGmNiwsLDaD1AppRoopyR+EfHESvofGWO+ckYMSinlqpwxqkeAt4EdxpiZtb1/pZRydc5o8Q8CbgBGiMhG+2ecE+JQSimXVOvDOY0xvwJS2/tVSill0Tt3lVLKxWjiV0opF6OJXymlXIwmfqWUcjGa+JVSysVo4ldKKRejiV8ppVyMJn6llHIxmviVUsrFaOJXSikXo4lfKaVcjCZ+pZRyMZr4lVLKxWjiV0opF6OJXymlXIwmfqWUcjGa+JVSysVo4ldKKRejiV8ppVyMJn6llHIxmviVUsrFaOJXSikXo4lfKaVcjCZ+pZRyMZr4lVLKxWjiV0opF6OJXymlXIwmfqWUcjGa+JVSysVo4ldKKRejiV8ppVyMJn6llHIxmviVUsrFaOJXSikXo4lfKaVcjFMSv4iMEZE4EdkjItOcEYNSSrmqWk/8IuIOvAKMBboC14pI19qOQymlXJUzWvz9gD3GmL3GmBzgU+ByJ8ShlFIuycMJ+2wFHCzyPhHoX3IhEbkNuM1+my4icRe4vybAiQtct77SOrsGrbNrqEqd25ZV6IzEL2WUmVIFxswCZlV5ZyJrjTGxVd1OfaJ1dg1aZ9dQE3V2RldPItC6yPtw4LAT4lBKKZfkjMS/BogUkXYi4gVcA8x3QhxKKeWSar2rxxiTJyJ/Bn4A3IF3jDHbanCXVe4uqoe0zq5B6+waqr3OYkyp7nWllFINmN65q5RSLkYTv1JKuZgGnfgb6tQQIvKOiBwXka1FykJEZJGI7LZ/Ny7y2SP2dxAnIqOdE/WFE5HWIvKziOwQkW0ico9d3pDr7CMiq0Vkk13np+zyBlvnQiLiLiIbROQb+32DrrOIJIjIFhHZKCJr7bKarbMxpkH+YF04jgfaA17AJqCrs+OqproNAXoDW4uU/ROYZr+eBvzDft3Vrrs30M7+TtydXYdK1rcF0Nt+HQjssuvVkOssQID92hNYBQxoyHUuUvf7gY+Bb+z3DbrOQALQpERZjda5Ibf4G+zUEMaYZcDJEsWXA+/Zr98DrihS/qkxJtsYsw/Yg/Xd1BvGmCPGmPX26zRgB9Yd4A25zsYYk26/9bR/DA24zgAiEg6MB94qUtyg61yOGq1zQ078ZU0N0cpJsdSGZsaYI2AlSqCpXd6gvgcRiQBisFrADbrOdpfHRuA4sMgY0+DrDPwbeAgoKFLW0OtsgIUiss6eqgZquM7OmLKhtlRoaggX0GC+BxEJAL4E7jXGnBYpq2rWomWU1bs6G2PygWgRaQTMFZHu51i83tdZRCYAx40x60RkWEVWKaOsXtXZNsgYc1hEmgKLRGTnOZatljo35Ba/q00NcUxEWgDYv4/b5Q3iexART6yk/5Ex5iu7uEHXuZAxJgVYAoyhYdd5EDBRRBKwumZHiMiHNOw6Y4w5bP8+DszF6rqp0To35MTvalNDzAdutF/fCMwrUn6NiHiLSDsgEljthPgumFhN+7eBHcaYmUU+ash1DrNb+oiILzAK2EkDrrMx5hFjTLgxJgLr/+tiY8z1NOA6i4i/iAQWvgYuBbZS03V29hXtGr5aPg5rBEg88Fdnx1ON9foEOALkYrUAbgFCgZ+A3fbvkCLL/9X+DuKAsc6O/wLqOxjrdHYzsNH+GdfA69wT2GDXeSvwhF3eYOtcov7DODuqp8HWGWvU4Sb7Z1thnqrpOuuUDUop5WIaclePUkqpMmjiV0opF6OJXymlXIwmfqWUcjGa+JVSysVo4lcuTUTy7VkRC3+qbRZXEYkoOoOqUnVFQ56yQamKyDTGRDs7CKVqk7b4lSqDPUf6P+w58VeLSEe7vK2I/CQim+3fbezyZiIy154/f5OIDLQ35S4ib9pz6i+078JFRKaKyHZ7O586qZrKRWniV67Ot0RXz9VFPjttjOkH/Bdr1kjs1+8bY3oCHwEv2eUvAUuNMb2wnpWwzS6PBF4xxnQDUoDf2+XTgBh7O3fUTNWUKpveuatcmoikG2MCyihPAEYYY/baE8QdNcaEisgJoIUxJtcuP2KMaSIiSUC4MSa7yDYisKZTjrTfPwx4GmOeEZHvgXTga+Brc3bufaVqnLb4lSqfKed1ecuUJbvI63zOXlcbD7wC9AHWiYheb1O1RhO/UuW7usjvFfbr5VgzRwJMBn61X/8E3AmOB6gElbdREXEDWhtjfsZ66EgjoNRZh1I1RVsZytX52k+5KvS9MaZwSKe3iKzCaiBda5dNBd4RkQeBJOAmu/weYJaI3ILVsr8TawbVsrgDH4pIMNaDNf5lrDn3laoV2sevVBnsPv5YY8wJZ8eiVHXTrh6llHIx2uJXSikXoy1+pZRyMZr4lVLKxWjiV0opF6OJXymlXIwmfqWUcjH/D97UTG4U5BAaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = len(all_losses)\n",
    "plt.plot(range(epochs), all_losses, label='Training Perplexity')\n",
    "plt.title('Training Perplexity Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.ylim(0, 10)\n",
    "\n",
    "# set the legend and display the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0ae96",
   "metadata": {},
   "source": [
    "Task 2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef648d3",
   "metadata": {},
   "source": [
    "Use the trained Char-Rnn model from Task 1 and report 3 generated sequences of length 100 by\n",
    "priming the model with random character sequences of length 5. E.g., \"2 b3n\", \"bg09Z\", etc.\n",
    "While reporting the generated sequences, also report their corresponding priming sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2add1927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hj34s like betterfeit you,\n",
      "And then the sead to past hate me other in me?\n",
      "\n",
      "WARWICK:\n",
      "Why close you, sir, I\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoder = torch.load('shakespeare.pt')\n",
    "print(generate(decoder, prime_str='hj34s', cuda=args.cuda, tensor_fn=char_tensor, chars=all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0da9c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2sf 4range to crusis.\n",
      "\n",
      "First Master:\n",
      "Alate one srather, 'tis twenty lord.\n",
      "\n",
      "ANTONIO:\n",
      "Where is yet you.\n",
      "\n",
      "CA\n"
     ]
    }
   ],
   "source": [
    "print(generate(decoder, prime_str='2sf 4', cuda=args.cuda, tensor_fn=char_tensor, chars=all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae1a70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3  LENTIO:\n",
      "Let the revieck'd; and thus last his clows. Where,\n",
      "Their bring it on all make me go talk\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "print(generate(decoder, prime_str='  3  ', cuda=args.cuda, tensor_fn=char_tensor, chars=all_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72624c7",
   "metadata": {},
   "source": [
    "Task 3 \n",
    "\n",
    "\n",
    "Like in the previous task, generate character sequences of length 100 for the following priming\n",
    "sequences:\n",
    "• The\n",
    "• What is\n",
    "• Shall I give\n",
    "• X087hNYB BHN BYFVuhsdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ff08ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ther and couns any sulipition.\n",
      "\n",
      "LEONTES:\n",
      "Benes,\n",
      "Wither's sentleman's discourph not affecting; to the ey\n"
     ]
    }
   ],
   "source": [
    "print(generate(decoder, prime_str='The', cuda=args.cuda, tensor_fn=char_tensor, chars=all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cc193eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is my care of York of the Frand:\n",
      "The Prove a sweet seen hard of reseetish'd the mifstors\n",
      "And she in th\n"
     ]
    }
   ],
   "source": [
    "print(generate(decoder, prime_str='What is', cuda=args.cuda, tensor_fn=char_tensor, chars=all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55c17231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I give must once? is my lord:\n",
      "I have done the morning them.\n",
      "\n",
      "ANGELO:\n",
      "Is not soul ship to me should king?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(decoder, prime_str='Shall I give', cuda=args.cuda, tensor_fn=char_tensor, chars=all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4513b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X087hNYB BHN BYFVuhsdbs cheeks often great further.\n",
      "\n",
      "GLOUCESTER:\n",
      "But you have you will be with you, sease:\n",
      "Let me them betw\n"
     ]
    }
   ],
   "source": [
    "print(generate(decoder, prime_str='X087hNYB BHN BYFVuhsdbs', cuda=args.cuda, tensor_fn=char_tensor, chars=all_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aecca71",
   "metadata": {},
   "source": [
    "Neural Language Models (OPTIONAL)\n",
    "\n",
    "1) What are Language Models? Where and how are Language Models used?\n",
    "\n",
    "Language Models (LMs) are computational models that are used to predict the probability of occurrence of a given sequence of words in a language. These models are trained on large datasets of text and are capable of predicting the likelihood of the next word in a sequence given the previous words.\n",
    "\n",
    "Language Models are used in various applications such as speech recognition, machine translation, text-to-speech synthesis, and automatic summarization.\n",
    "\n",
    "For example, language models are used in chatbots to generate responses to user queries. + in search engines to accurately predict the user’s next search query based on their previous searches.\n",
    "\n",
    "2) How can you use a trained Char-RNN model as a Character Level Language Model? \n",
    "A trained Char-RNN model can be used as a Character Level Language Model by inputting a sequence of characters and using the model to generate the probability distribution of the next character in the sequence. This can be achieved by passing the current sequence of characters through the model and using the output of the last layer to predict the probability distribution of the next character.\n",
    "\n",
    "Once the probability distribution is generated, the next character can be sampled from the distribution either deterministically by selecting the character with the highest probability or stochastically by sampling from the distribution according to the corresponding probabilities. By repeating this process, a new sequence of characters can be generated.\n",
    "\n",
    "-> a powerful tool for generating new sequences of characters that can be used in various applications, such as text generation, language translation, and speech recognition.\n",
    "\n",
    "3) How can you train a Word Level Language Model? \n",
    "- Collect a large corpus of text data and preprocess it to remove unwanted characters, numbers, and punctuation.\n",
    "\n",
    "- Tokenize the text into words and create a vocabulary of all unique words present in the corpus.\n",
    "\n",
    "- Split the corpus into training and validation sets.\n",
    "\n",
    "- Convert the words into numerical representations using one-hot encoding or word embeddings.\n",
    "\n",
    "- Design and build a neural network architecture for the language model, which can take in a sequence of word embeddings and predict the next word in the sequence.\n",
    "\n",
    "- Train the language model on the training data, minimizing the loss function, and validate the model on the validation set.\n",
    "\n",
    "- Tune the hyperparameters of the model\n",
    "\n",
    "Once the model is trained, it can be used to generate new text by sampling from the output probabilities of the last layer of the model.\n",
    "\n",
    "4) Formally describe the inference model that an RNN trained to predict the next word represents?\n",
    "\n",
    "The language model takes in a sequence of words (w_1, w_2, ..., w_t), and its goal is to predict the probability distribution of the next word w_t+1.\n",
    "\n",
    "The RNN-based language model uses the hidden state h_t as a summary of the sequence up to time step t, and this is updated after each new input. The hidden state at time t is a function of the current input word w_t, the previous hidden state h_t-1, and the network parameters, such as the weights and biases.\n",
    "\n",
    "The conditional probability distribution of the next word w_t+1 given the previous words up to time step t, p(w_t+1|w_1, w_2, ..., w_t), can be estimated using the current hidden state h_t and the network parameters.\n",
    "\n",
    "During inference, the RNN generates a sequence of words by iteratively predicting the probabilities of the next word and sampling from the distribution. This process continues until a predetermined stopping criteria is met, such as a predefined maximum sequence length, or a specific end-of-sequence token is generated.\n",
    "\n",
    "Overall, an RNN trained to predict the next word represents a language model that estimates the probability distribution of the next word given the previous words, using a sequence summary of the inputs up to the current time step. \n",
    "\n",
    "5) How will you generate the \"probability of existence\" of an input sequence of words, given a trained RNN Language Model? \n",
    "\n",
    "We can use the trained RNN to generate the sequence of hidden states corresponding to each word in the input sequence. Then, for each time step, the output of the RNN can be passed through a softmax function to obtain the probability distribution over the vocabulary of possible next words.\n",
    "\n",
    "For example, if we have an input sequence of words \"The cat sat on the\", we could pass the sequence through the trained RNN and compute the probability of each possible next word (e.g. \"mat,\" \"chair,\" \"car\"). We could then multiply these probabilities together to get the overall probability of the input sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57d84c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
